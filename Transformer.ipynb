{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7376e37-79da-4d01-b120-aaec6cfe9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e44fd0a-6775-47b4-99f7-bc55e1ae2336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/zynicide/wine-reviews\n",
      "Wine Reviews dataset downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Install the Kaggle API client\n",
    "# Uncomment the following line if you haven't installed the Kaggle API client\n",
    "# !pip install kaggle\n",
    "\n",
    "# Step 2: Set up the environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = \"\"\n",
    "os.environ['KAGGLE_KEY'] = \"\"\n",
    "\n",
    "# Step 3: Use the Kaggle API to download the Wine Reviews dataset\n",
    "# The identifier for the Wine Reviews dataset\n",
    "dataset_identifier = 'zynicide/wine-reviews'\n",
    "\n",
    "# Download and unzip the dataset\n",
    "kaggle.api.dataset_download_files(dataset_identifier, path='./wine_reviews', unzip=True)\n",
    "\n",
    "print(\"Wine Reviews dataset downloaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4d53f1e-866e-46a1-9951-a5062a494e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine descriptions loaded successfully\n",
      "[\"Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\", \"This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.\", 'Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.', 'Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.', \"Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal characteristics. Nonetheless, if you think of it as a pleasantly unfussy country wine, it's a good companion to a hearty winter stew.\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the CSV file\n",
    "# The dataset is expected to be in the directory './wine_reviews' with the filename 'winemag-data-130k-v2.csv'\n",
    "wine_reviews_df = pd.read_csv('./wine_reviews/winemag-data-130k-v2.csv')\n",
    "\n",
    "# Create a list containing the text descriptions of each wine\n",
    "# The 'description' column in the DataFrame contains the text descriptions of the wines\n",
    "wine_descriptions = wine_reviews_df['description'].tolist()\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Wine descriptions loaded successfully\")\n",
    "\n",
    "# Print the first few descriptions to verify\n",
    "print(wine_descriptions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d39ca025-6ae1-4c32-8a3c-6faa383646a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation padded successfully\n",
      "[\"Aromas include tropical fruit ,  broom ,  brimstone and dried herb .  The palate isn ' t overly expressive ,  offering unripened apple ,  citrus and dried sage alongside brisk acidity . \", \"This is ripe and fruity ,  a wine that is smooth while still structured .  Firm tannins are filled out with juicy red berry fruits and freshened with acidity .  It ' s  already drinkable ,  although it will certainly be better from 2016 . \", 'Tart and snappy ,  the flavors of lime flesh and rind dominate .  Some green pineapple pokes through ,  with crisp acidity underscoring the flavors .  The wine was all stainless - steel fermented . ', 'Pineapple rind ,  lemon pith and orange blossom start off the aromas .  The palate is a bit more opulent ,  with notes of honey - drizzled guava and mango giving way to a slightly astringent ,  semidry finish . ', \"Much like the regular bottling from 2012 ,  this comes across as rather rough and tannic ,  with rustic ,  earthy ,  herbal characteristics .  Nonetheless ,  if you think of it as a pleasantly unfussy country wine ,  it ' s a good companion to a hearty winter stew . \"]\n"
     ]
    }
   ],
   "source": [
    "def pad_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function takes a string and pads punctuation marks with spaces\n",
    "    so that each punctuation mark is treated as a separate word.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text string to process.\n",
    "    \n",
    "    Returns:\n",
    "    str: The processed text string with padded punctuation.\n",
    "    \"\"\"\n",
    "    # Iterate over each punctuation mark defined in the string.punctuation constant\n",
    "    for punctuation in string.punctuation:\n",
    "        # Replace the punctuation mark with the same mark padded with spaces\n",
    "        text = text.replace(punctuation, f' {punctuation} ')\n",
    "    return text\n",
    "\n",
    "# Apply the padding function to each wine description in the list\n",
    "# This will create a new list with padded descriptions\n",
    "padded_wine_descriptions = [pad_punctuation(description) for description in wine_descriptions]\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Punctuation padded successfully\")\n",
    "\n",
    "# Print the first few padded descriptions to verify\n",
    "print(padded_wine_descriptions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fcdeaee-8de2-4485-8e8e-312fb9cd3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and padding/clipping completed successfully\n",
      "tf.Tensor(\n",
      "[[   18   864   202    15  1978  3610     2    90   107     3    19   898\n",
      "    197  1018   919   307 14577    49    63     2    90   419   141   359\n",
      "     20     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    7     8    28     2    77     4    10    17     8   101    62   126\n",
      "    144    80    24    29  1050    92     6    74    32    43    50     2\n",
      "   3739     6    20     9    14   375   695   292     9    89   676    98\n",
      "    439    22   420     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  116     2   924     3    11     5   127   941     2   613   557    81\n",
      "     75   199  6179    66     6    55    20  5571     3    11     3    10\n",
      "    388   103   806   728   480     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  199   613    76   787     2   129   442   789   196     3    18     3\n",
      "     19     8     4   128    73   518     6    35     5   186  3363  1131\n",
      "      2   523   380   294    13     4   155   495  3204    21     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  259    87     3  1349   216    22  1067     7   243   543    36   337\n",
      "    790     2   115     6   500   152   118  1132  1638   387   150  1827\n",
      "      5     9    36     4  1043  4583  1985    10     9    14     4    79\n",
      "   1856    13     4   786  3097  2870     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]], shape=(5, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the TextVectorization layer\n",
    "max_tokens = 20000  # Maximum number of tokens to keep, based on word frequency\n",
    "output_sequence_length = 100  # Length to pad or clip the output sequences\n",
    "\n",
    "# Create the TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=output_sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to the padded descriptions\n",
    "vectorize_layer.adapt(padded_wine_descriptions)\n",
    "\n",
    "# Apply the TextVectorization layer to the padded descriptions\n",
    "vectorized_wine_descriptions = vectorize_layer(padded_wine_descriptions)\n",
    "\n",
    "print(\"Tokenization and padding/clipping completed successfully\")\n",
    "# Print the first few tokenized and padded/clipped descriptions to verify\n",
    "print(vectorized_wine_descriptions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c04b5cf7-1acf-4078-9ffb-e466b83a077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: (129971, 99)\n",
      "Outputs shape: (129971, 99)\n",
      "First few input sequences:\n",
      "tf.Tensor(\n",
      "[[   18   864   202    15  1978  3610     2    90   107     3    19   898\n",
      "    197  1018   919   307 14577    49    63     2    90   419   141   359\n",
      "     20     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    7     8    28     2    77     4    10    17     8   101    62   126\n",
      "    144    80    24    29  1050    92     6    74    32    43    50     2\n",
      "   3739     6    20     9    14   375   695   292     9    89   676    98\n",
      "    439    22   420     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [  116     2   924     3    11     5   127   941     2   613   557    81\n",
      "     75   199  6179    66     6    55    20  5571     3    11     3    10\n",
      "    388   103   806   728   480     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [  199   613    76   787     2   129   442   789   196     3    18     3\n",
      "     19     8     4   128    73   518     6    35     5   186  3363  1131\n",
      "      2   523   380   294    13     4   155   495  3204    21     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [  259    87     3  1349   216    22  1067     7   243   543    36   337\n",
      "    790     2   115     6   500   152   118  1132  1638   387   150  1827\n",
      "      5     9    36     4  1043  4583  1985    10     9    14     4    79\n",
      "   1856    13     4   786  3097  2870     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]], shape=(5, 99), dtype=int64)\n",
      "First few output sequences:\n",
      "tf.Tensor(\n",
      "[[  864   202    15  1978  3610     2    90   107     3    19   898   197\n",
      "   1018   919   307 14577    49    63     2    90   419   141   359    20\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    8    28     2    77     4    10    17     8   101    62   126   144\n",
      "     80    24    29  1050    92     6    74    32    43    50     2  3739\n",
      "      6    20     9    14   375   695   292     9    89   676    98   439\n",
      "     22   420     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    2   924     3    11     5   127   941     2   613   557    81    75\n",
      "    199  6179    66     6    55    20  5571     3    11     3    10   388\n",
      "    103   806   728   480     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [  613    76   787     2   129   442   789   196     3    18     3    19\n",
      "      8     4   128    73   518     6    35     5   186  3363  1131     2\n",
      "    523   380   294    13     4   155   495  3204    21     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   87     3  1349   216    22  1067     7   243   543    36   337   790\n",
      "      2   115     6   500   152   118  1132  1638   387   150  1827     5\n",
      "      9    36     4  1043  4583  1985    10     9    14     4    79  1856\n",
      "     13     4   786  3097  2870     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]], shape=(5, 99), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create inputs and outputs for training\n",
    "def create_training_data(vectorized_texts):\n",
    "    \"\"\"\n",
    "    This function creates a training set where the inputs are the tokenized text strings\n",
    "    and the outputs to predict are the same strings shifted by one token.\n",
    "\n",
    "    Parameters:\n",
    "    vectorized_texts (tf.Tensor): The tokenized and padded text strings.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the inputs and outputs for training.\n",
    "    \"\"\"\n",
    "    inputs = vectorized_texts[:, :-1]  # All tokens except the last one\n",
    "    outputs = vectorized_texts[:, 1:]  # All tokens except the first one\n",
    "    return inputs, outputs\n",
    "\n",
    "# Apply the function to the vectorized descriptions\n",
    "inputs, outputs = create_training_data(vectorized_wine_descriptions)\n",
    "\n",
    "# Print the shapes of the inputs and outputs to verify\n",
    "print(f\"Inputs shape: {inputs.shape}\")\n",
    "print(f\"Outputs shape: {outputs.shape}\")\n",
    "\n",
    "# Print the first few examples to verify\n",
    "print(\"First few input sequences:\")\n",
    "print(inputs[:5])\n",
    "print(\"First few output sequences:\")\n",
    "print(outputs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22929573-1c19-4a65-8028-a01ef729e73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ query (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ value (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,016</span> │ query[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ query[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ value[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ query (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ value (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,016\u001b[0m │ query[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ query[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ value[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">198,016</span> (773.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m198,016\u001b[0m (773.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">198,016</span> (773.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m198,016\u001b[0m (773.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shapes\n",
    "query_input = Input(shape=(None, 128), name='query')\n",
    "value_input = Input(shape=(None, 128), name='value')  # Should match key_dim for keys/queries\n",
    "\n",
    "# Create a MultiHeadAttention layer\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    num_heads=4,\n",
    "    key_dim=128,\n",
    "    value_dim=64,\n",
    "    name='multi_head_attention'\n",
    ")\n",
    "\n",
    "# Apply the MultiHeadAttention layer\n",
    "attention_output = multi_head_attention(query=query_input, value=value_input, key=query_input)\n",
    "\n",
    "# Define a simple model\n",
    "model = Model(inputs=[query_input, value_input], outputs=attention_output)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4919793-18c9-4a86-9d28-21865f0bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Attention Mask (1 batch, 10 dest, 10 src):\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a causal attention mask to be used in a multi-head attention layer.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The number of batches.\n",
    "    - n_dest (int): The size of the destination sequence (target sequence length).\n",
    "    - n_src (int): The size of the source sequence (input sequence length).\n",
    "    - dtype (tf.DType): The data type of the mask tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tf.Tensor: A causal attention mask of shape (batch_size, n_dest, n_src).\n",
    "    \"\"\"\n",
    "    # Create range tensors for destination and source\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    \n",
    "    # Create a mask where positions i >= j are set to True\n",
    "    m = i >= j - n_src + n_dest\n",
    "    \n",
    "    # Cast the mask to the desired dtype\n",
    "    mask = tf.cast(m, dtype)\n",
    "    \n",
    "    # Reshape the mask to add a batch dimension\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    \n",
    "    # Create a multiplier for tiling the mask to match the batch size\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    \n",
    "    # Tile the mask to match the batch size\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "# Testing the causal_attention_mask function\n",
    "mask = causal_attention_mask(1, 10, 10, dtype=tf.int32)\n",
    "print(\"Causal Attention Mask (1 batch, 10 dest, 10 src):\")\n",
    "print(np.transpose(mask[0]))\n",
    "\n",
    "# Ensure the mask is as expected for visualization purposes\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(np.transpose(mask[0].numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a60dadaa-8532-4d6c-9c3b-28e5e96c146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Ensure the causal_attention_mask function is defined\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a causal attention mask to be used in a multi-head attention layer.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The number of batches.\n",
    "    - n_dest (int): The size of the destination sequence (target sequence length).\n",
    "    - n_src (int): The size of the source sequence (input sequence length).\n",
    "    - dtype (tf.DType): The data type of the mask tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tf.Tensor: A causal attention mask of shape (batch_size, n_dest, n_src).\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerBlock layer.\n",
    "\n",
    "        Parameters:\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        - key_dim (int): Size of each attention head for query and key.\n",
    "        - embed_dim (int): Dimensionality of the output space.\n",
    "        - ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        - dropout_rate (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=key_dim, value_dim=key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Dropout and normalization layers\n",
    "        self.dropout_1 = layers.Dropout(dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(embed_dim)\n",
    "        \n",
    "        # Dropout and normalization layers\n",
    "        self.dropout_2 = layers.Dropout(dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the TransformerBlock.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (tf.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "        - tf.Tensor: Output tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        - tf.Tensor: Attention scores of shape (batch_size, num_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        \n",
    "        # Create a causal mask\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        \n",
    "        # Apply dropout and layer normalization\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "        \n",
    "        # Apply feed-forward network\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        \n",
    "        # Apply layer normalization and return the output and attention scores\n",
    "        return self.ln_2(out1 + ffn_output), attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47502c3e-9242-4bd6-be60-a2233985377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Parameters:\n",
    "        - maxlen (int): Maximum length of the input sequences.\n",
    "        - vocab_size (int): Size of the vocabulary.\n",
    "        - embed_dim (int): Dimensionality of the embeddings.\n",
    "        \"\"\"\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Embedding layer for token embeddings\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size,  # Vocabulary size\n",
    "            output_dim=embed_dim  # Embedding dimension\n",
    "        )\n",
    "        \n",
    "        # Embedding layer for positional embeddings\n",
    "        self.pos_emb = layers.Embedding(\n",
    "            input_dim=maxlen,  # Maximum length of the sequence\n",
    "            output_dim=embed_dim  # Embedding dimension\n",
    "        )\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x (tf.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "        - tf.Tensor: Output tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        \"\"\"\n",
    "        # Get the length of the input sequences (seq_len)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        \n",
    "        # Create a tensor of positions [0, 1, 2, ..., maxlen-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        \n",
    "        # Apply position embedding to the positions tensor\n",
    "        positions = self.pos_emb(positions)\n",
    "        \n",
    "        # Apply token embedding to the input tensor x\n",
    "        x = self.token_emb(x)\n",
    "        \n",
    "        # Add the token embeddings and position embeddings\n",
    "        # The broadcasting mechanism will add the positional embeddings to each sequence in the batch\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3efedb-ef8e-4732-ba51-ef7374bb3666",
   "metadata": {},
   "source": [
    "Now we are ready to build and train our GPT model! To put everything together, we\n",
    "need to pass our input text through the token and position embedding layer, then\n",
    "through our Transformer block. The final output of the network is a simple Dense\n",
    "layer with softmax activation over the number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05718481-c0df-4482-92fe-0588a6f9d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, losses\n",
    "\n",
    "MAX_LEN = 80\n",
    "VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 256\n",
    "N_HEADS = 2\n",
    "KEY_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
    ")(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "\n",
    "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "gpt.compile(optimizer=\"adam\", loss=[losses.SparseCategoricalCrossentropy(from_logits=True), None])\n",
    "\n",
    "# Prepare the dataset for training\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "# Create TensorFlow Dataset from the processed inputs and outputs\n",
    "train_inputs, train_outputs = create_training_data(vectorized_wine_descriptions)\n",
    "\n",
    "# Ensure label values are within the vocabulary range\n",
    "train_inputs = np.clip(train_inputs, 0, VOCAB_SIZE - 1)\n",
    "train_outputs = np.clip(train_outputs, 0, VOCAB_SIZE - 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_inputs, train_outputs))\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Use callbacks for early stopping and model checkpointing\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('gpt_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "gpt.fit(train_ds, epochs=5, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6fbaa-e3d0-47d7-88d4-af892bb4a976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
